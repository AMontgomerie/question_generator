{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"qa_evaluator_training.ipynb","provenance":[],"mount_file_id":"1emXX3VUG1HfrPUk65Tcyabxc_1I5nJhb","authorship_tag":"ABX9TyN8wqt3/+nsjX3evnKkRc+T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7b869b46c31e41289d40cd3001e54211":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_15ff67a5796b40f7be6f7cdb9ca4b66b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4745549f016a40afbcae67331f03c114","IPY_MODEL_2db5b2c6469a4ba2bcde107554cdb336"]}},"15ff67a5796b40f7be6f7cdb9ca4b66b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4745549f016a40afbcae67331f03c114":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_77390ebec1fa4eb88b067cbc0a15050b","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":213450,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":213450,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_798d7173956b49f8b884503c94cb3c57"}},"2db5b2c6469a4ba2bcde107554cdb336":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ac9f5f79b82a48e7b6fb216d225d1e1a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 213k/213k [00:00&lt;00:00, 703kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2dbbadfefdaf41b281658b0e46dad864"}},"77390ebec1fa4eb88b067cbc0a15050b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"798d7173956b49f8b884503c94cb3c57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ac9f5f79b82a48e7b6fb216d225d1e1a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2dbbadfefdaf41b281658b0e46dad864":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6c54d257da264b329514e314326658c5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2fca23a9f1b849a78cb247959fe12fc3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2e750521e6114a30966a938ae680b841","IPY_MODEL_74a6f036c022439798bb16ad9eff462d"]}},"2fca23a9f1b849a78cb247959fe12fc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2e750521e6114a30966a938ae680b841":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_59966811ca0e400b9cdca941811138e5","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":433,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":433,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e9829161178448338a0bc08cb30c8f23"}},"74a6f036c022439798bb16ad9eff462d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5b24cdf571274e76b8729f4223e7c52f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 433/433 [00:11&lt;00:00, 36.2B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_829f60c11d42433787333dfc8ed8887c"}},"59966811ca0e400b9cdca941811138e5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e9829161178448338a0bc08cb30c8f23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5b24cdf571274e76b8729f4223e7c52f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"829f60c11d42433787333dfc8ed8887c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"552a42a9adfc42f89e79364d2b744286":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5ad655b4563e4fb8844e09c6d0a43bd5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_19d7cac1687c401d87a05ed7a8ab690b","IPY_MODEL_4c2417ac0c364626b157f343f62f7491"]}},"5ad655b4563e4fb8844e09c6d0a43bd5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"19d7cac1687c401d87a05ed7a8ab690b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_58db1aa5bf234d6788e452f1a8d8e484","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":435779157,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":435779157,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f675811ee5864bb18425949215aba4bd"}},"4c2417ac0c364626b157f343f62f7491":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f3842325875a446683b84ff4692bab07","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 436M/436M [00:11&lt;00:00, 38.0MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6331b435b7da4245a690288cd84dbe4c"}},"58db1aa5bf234d6788e452f1a8d8e484":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f675811ee5864bb18425949215aba4bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f3842325875a446683b84ff4692bab07":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6331b435b7da4245a690288cd84dbe4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"zYh3QRoOX5pt","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595029664256,"user_tz":-540,"elapsed":4109,"user":{"displayName":"Adam Montgomerie","photoUrl":"","userId":"08195438125971362273"}}},"source":["import os\n","import sys\n","import random\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"rCrFsm-Ucw1L","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":588},"executionInfo":{"status":"ok","timestamp":1595029673250,"user_tz":-540,"elapsed":12803,"user":{"displayName":"Adam Montgomerie","photoUrl":"","userId":"08195438125971362273"}},"outputId":"77fdec41-b19c-4f4d-a62d-d2802c6c1138"},"source":["!pip install transformers"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n","\r\u001b[K     |▍                               | 10kB 7.4MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 3.2MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30kB 4.3MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40kB 3.6MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51kB 4.3MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61kB 4.8MB/s eta 0:00:01\r\u001b[K     |███                             | 71kB 5.2MB/s eta 0:00:01\r\u001b[K     |███▍                            | 81kB 5.7MB/s eta 0:00:01\r\u001b[K     |███▉                            | 92kB 5.7MB/s eta 0:00:01\r\u001b[K     |████▎                           | 102kB 5.7MB/s eta 0:00:01\r\u001b[K     |████▊                           | 112kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 122kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 133kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 143kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 153kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 163kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 174kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 184kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 194kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 204kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 215kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 225kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 235kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 245kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 256kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 266kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 276kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 286kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 296kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 307kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 317kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 327kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 337kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 348kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 358kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 368kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 378kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 389kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 399kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 409kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 419kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 430kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 440kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 450kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 460kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 471kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 481kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 491kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 501kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 512kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 522kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 532kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 542kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 552kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 563kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 573kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 583kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 593kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 604kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 614kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 624kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 634kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 645kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 655kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 665kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 675kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 686kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 696kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 706kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 716kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 727kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 737kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 747kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 757kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 768kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 778kB 5.7MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Collecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 18.1MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Collecting tokenizers==0.8.1.rc1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 40.3MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 41.8MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=ef75e3468d7ff4a4f4b44a007e4e9bd73c9b8c80660407b43b6cc8b64bf2c80a\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MoVZz2ipcyED","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595029675601,"user_tz":-540,"elapsed":13737,"user":{"displayName":"Adam Montgomerie","photoUrl":"","userId":"08195438125971362273"}}},"source":["from transformers import BertTokenizer\n","from transformers import BertForSequenceClassification\n","from transformers import BertConfig"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"FZ7-oYxKbShb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1595029735100,"user_tz":-540,"elapsed":72374,"user":{"displayName":"Adam Montgomerie","photoUrl":"","userId":"08195438125971362273"}},"outputId":"653092af-3375-452b-ab12-0c48ca1cee9c"},"source":["!pip install -U spacy[cuda92]\n","!python -m spacy download en_core_web_sm\n","import spacy\n","import en_core_web_sm\n","spacy.prefer_gpu()\n","spacy_nlp = en_core_web_sm.load()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting spacy[cuda92]\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/b5/c7a92c7ce5d4b353b70b4b5b4385687206c8b230ddfe08746ab0fd310a3a/spacy-2.3.2-cp36-cp36m-manylinux1_x86_64.whl (9.9MB)\n","\u001b[K     |████████████████████████████████| 10.0MB 5.1MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda92]) (2.23.0)\n","Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda92]) (1.0.0)\n","Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda92]) (1.1.3)\n","Collecting thinc==7.4.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/ae/ef3ae5e93639c0ef8e3eb32e3c18341e511b3c515fcfc603f4b808087651/thinc-7.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 38.8MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda92]) (1.0.2)\n","Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda92]) (2.0.3)\n","Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda92]) (0.7.0)\n","Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda92]) (3.0.2)\n","Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda92]) (1.0.2)\n","Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda92]) (4.41.1)\n","Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy[cuda92]) (49.1.0)\n","Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda92]) (1.18.5)\n","Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda92]) (0.4.1)\n","Collecting cupy-cuda92<9.0.0,>=5.0.0b4; extra == \"cuda92\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/ed/7aee0f78919d02b5f607f62a1abe9ca3a4a7c3bdc55099ed910a58d3972e/cupy_cuda92-8.0.0b4-cp36-cp36m-manylinux1_x86_64.whl (325.1MB)\n","\u001b[K     |████████████████████████████████| 325.1MB 32kB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda92]) (2020.6.20)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda92]) (1.24.3)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda92]) (3.0.4)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda92]) (2.10)\n","Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy[cuda92]) (1.7.0)\n","Requirement already satisfied, skipping upgrade: fastrlock>=0.3 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda92<9.0.0,>=5.0.0b4; extra == \"cuda92\"->spacy[cuda92]) (0.5)\n","Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy[cuda92]) (3.1.0)\n","Installing collected packages: thinc, cupy-cuda92, spacy\n","  Found existing installation: thinc 7.4.0\n","    Uninstalling thinc-7.4.0:\n","      Successfully uninstalled thinc-7.4.0\n","  Found existing installation: spacy 2.2.4\n","    Uninstalling spacy-2.2.4:\n","      Successfully uninstalled spacy-2.2.4\n","Successfully installed cupy-cuda92-8.0.0b4 spacy-2.3.2 thinc-7.4.1\n","Collecting en_core_web_sm==2.3.1\n","\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0MB)\n","\u001b[K     |████████████████████████████████| 12.1MB 379kB/s \n","\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.3.1) (2.3.2)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.18.5)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.1.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.23.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.41.1)\n","Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.1.0)\n","Building wheels for collected packages: en-core-web-sm\n","  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-cp36-none-any.whl size=12047111 sha256=ba91120bef5ebaf18c094d5a3966c02cb019d34a4a414d9a6bfd4337e2488ebd\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-3i0tjict/wheels/2b/3f/41/f0b92863355c3ba34bb32b37d8a0c662959da0058202094f46\n","Successfully built en-core-web-sm\n","Installing collected packages: en-core-web-sm\n","  Found existing installation: en-core-web-sm 2.2.5\n","    Uninstalling en-core-web-sm-2.2.5:\n","      Successfully uninstalled en-core-web-sm-2.2.5\n","Successfully installed en-core-web-sm-2.3.1\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lIW-LTWri3PT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595029736392,"user_tz":-540,"elapsed":1260,"user":{"displayName":"Adam Montgomerie","photoUrl":"","userId":"08195438125971362273"}},"outputId":"d0c0cfb3-d2a0-47c2-f69d-ccef8ccc2fc5"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Using device: cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hqayKzYSc5tg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["7b869b46c31e41289d40cd3001e54211","15ff67a5796b40f7be6f7cdb9ca4b66b","4745549f016a40afbcae67331f03c114","2db5b2c6469a4ba2bcde107554cdb336","77390ebec1fa4eb88b067cbc0a15050b","798d7173956b49f8b884503c94cb3c57","ac9f5f79b82a48e7b6fb216d225d1e1a","2dbbadfefdaf41b281658b0e46dad864"]},"executionInfo":{"status":"ok","timestamp":1595029748507,"user_tz":-540,"elapsed":13357,"user":{"displayName":"Adam Montgomerie","photoUrl":"","userId":"08195438125971362273"}},"outputId":"375611d9-0e87-4d87-bab1-c4a36ab68096"},"source":["DIR = \"/content/drive/My Drive/ml_hw/NLP/question_generator/\"\n","PRETRAINED_MODEL = 'bert-base-cased'\n","BATCH_SIZE = 16\n","SEQ_LENGTH = 512\n","\n","tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL)\n","\n","class QAEvalDataset(Dataset):\n","    def __init__(self, csv):\n","        self.df = pd.read_csv(csv, engine='python')\n","        self.transforms = [self.shuffle, self.corrupt]\n","\n","    def __len__(self):\n","         return len(self.df)\n","\n","    def __getitem__(self, idx): \n","        _, question, answer = self.df.iloc[idx]\n","        label = random.choice([0, 1])\n","\n","        if label == 0:\n","            question, answer = random.choice(self.transforms)(question, answer)\n","\n","        encoded_data = tokenizer(\n","            text=question,\n","            text_pair=answer,\n","            pad_to_max_length=True, \n","            max_length=SEQ_LENGTH,\n","            truncation=True,\n","            return_tensors=\"pt\"\n","        )\n","\n","        encoded_data['input_ids'] = torch.squeeze(encoded_data['input_ids'])\n","        encoded_data['token_type_ids'] = torch.squeeze(encoded_data['token_type_ids'])\n","        encoded_data['attention_mask'] = torch.squeeze(encoded_data['attention_mask'])\n","        return (encoded_data.to(device), torch.tensor(label).to(device))\n","    \n","    def shuffle(self, question, answer):\n","        shuffled_answer = answer\n","        while shuffled_answer == answer:\n","            shuffled_answer = self.df.sample(1)['answer'].item()\n","        return question, shuffled_answer\n","    \n","    def corrupt(self, question, answer):\n","        doc = spacy_nlp(question)\n","        if len(doc.ents) > 1:\n","            # Replace all entities in the sentence with the same thing\n","            copy_ent = str(random.choice(doc.ents))\n","            for ent in doc.ents:\n","                question = question.replace(str(ent), copy_ent)\n","        elif len(doc.ents) == 1:\n","            # Replace the answer with an entity from the question\n","            answer = str(doc.ents[0])\n","        else:\n","            question, answer = self.shuffle(question, answer)\n","        return question, answer\n","\n","\n","train_set = QAEvalDataset(os.path.join(DIR, 'qa_eval_train_2.csv')) \n","train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n","valid_set = QAEvalDataset(os.path.join(DIR, 'qa_eval_valid_2.csv')) \n","valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False)"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7b869b46c31e41289d40cd3001e54211","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8E4jQkWdolsT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":218,"referenced_widgets":["6c54d257da264b329514e314326658c5","2fca23a9f1b849a78cb247959fe12fc3","2e750521e6114a30966a938ae680b841","74a6f036c022439798bb16ad9eff462d","59966811ca0e400b9cdca941811138e5","e9829161178448338a0bc08cb30c8f23","5b24cdf571274e76b8729f4223e7c52f","829f60c11d42433787333dfc8ed8887c","552a42a9adfc42f89e79364d2b744286","5ad655b4563e4fb8844e09c6d0a43bd5","19d7cac1687c401d87a05ed7a8ab690b","4c2417ac0c364626b157f343f62f7491","58db1aa5bf234d6788e452f1a8d8e484","f675811ee5864bb18425949215aba4bd","f3842325875a446683b84ff4692bab07","6331b435b7da4245a690288cd84dbe4c"]},"executionInfo":{"status":"ok","timestamp":1595029781310,"user_tz":-540,"elapsed":46149,"user":{"displayName":"Adam Montgomerie","photoUrl":"","userId":"08195438125971362273"}},"outputId":"ecefa1ca-cdb9-48dd-cb7d-c3e196dc2057"},"source":["LR = 0.001\n","EPOCHS = 10\n","LOG_INTERVAL = 500\n","\n","model = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL)\n","model = model.to(device)\n","optimizer = torch.optim.SGD(model.parameters(), lr=LR)"],"execution_count":8,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6c54d257da264b329514e314326658c5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"552a42a9adfc42f89e79364d2b744286","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"CKcR_AClq6IG","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595029781312,"user_tz":-540,"elapsed":46146,"user":{"displayName":"Adam Montgomerie","photoUrl":"","userId":"08195438125971362273"}}},"source":["SAVED_MODEL_PATH = \"/content/drive/My Drive/ml_hw/NLP/question_generator/qa_eval_model_trained_2.pth\"\n","\n","def train():\n","    model.train()\n","    total_loss = 0.\n","    for batch_index, batch in enumerate(train_loader):\n","        data, labels = batch\n","        optimizer.zero_grad()\n","        output = model(**data, labels=labels)\n","        loss = output[0]\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n","        optimizer.step()\n","        total_loss += loss.item()\n","        \n","        if batch_index % LOG_INTERVAL == 0 and batch_index > 0:\n","            cur_loss = total_loss / LOG_INTERVAL\n","            print('| epoch {:3d} | ' \n","                  '{:5d}/{:5d} batches | '\n","                  'loss {:5.2f}'.format(\n","                    epoch, \n","                    batch_index, len(train_loader), \n","                    cur_loss))\n","            total_loss = 0\n","\n","def evaluate(eval_model, data_loader):\n","    eval_model.eval()\n","    total_score = 0.\n","    with torch.no_grad():\n","        for batch_index, batch in enumerate(data_loader):\n","            data, labels = batch\n","            output = eval_model(**data, labels=labels)\n","            preds = np.argmax(output[1].cpu(), axis=1)\n","            total_score += (preds == labels.cpu()).sum()\n","    return total_score / (len(data_loader) * BATCH_SIZE)\n","\n","def save(epoch, model_state_dict, optimizer_state_dict, loss):\n","    torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model_state_dict,\n","            'optimizer_state_dict': optimizer_state_dict,\n","            'best_loss': loss,\n","            }, SAVED_MODEL_PATH)\n","\n","    print(\"| Model saved.\")\n","    print_line()\n","\n","def load():\n","    return torch.load(SAVED_MODEL_PATH)\n","\n","def print_line():\n","    LINE_WIDTH = 60\n","    print('-' * LINE_WIDTH)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"vGzxEzlPjjwp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"7455ebf0-603b-442a-ae47-cb2fbd4a5259"},"source":["highest_accuracy = 0\n","\n","accuracy = evaluate(model, valid_loader)\n","print_line()\n","print('| Before training | accuracy on valid set: {:5.2f}%'.format(accuracy))\n","print_line()\n","\n","for epoch in range(1, EPOCHS + 1):\n","\n","    train()\n","    accuracy = evaluate(model, valid_loader)\n","    print_line()\n","    print('| end of epoch {:3d} | accuracy on valid set: {:5.2f}%'.format(\n","        epoch,\n","        accuracy)\n","    )\n","    print_line()\n","\n","    if accuracy > highest_accuracy:\n","        highest_accuracy = accuracy\n","        save(\n","             epoch, \n","             model.state_dict(), \n","             optimizer.state_dict(), \n","             highest_accuracy\n","        )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["------------------------------------------------------------\n","| Before training | accuracy on valid set:  0.66%\n","------------------------------------------------------------\n","| epoch   1 |   500/13007 batches | loss  0.58\n","| epoch   1 |  1000/13007 batches | loss  0.54\n","| epoch   1 |  1500/13007 batches | loss  0.51\n","| epoch   1 |  2000/13007 batches | loss  0.46\n","| epoch   1 |  2500/13007 batches | loss  0.42\n","| epoch   1 |  3000/13007 batches | loss  0.41\n","| epoch   1 |  3500/13007 batches | loss  0.40\n","| epoch   1 |  4000/13007 batches | loss  0.39\n","| epoch   1 |  4500/13007 batches | loss  0.37\n","| epoch   1 |  5000/13007 batches | loss  0.37\n","| epoch   1 |  5500/13007 batches | loss  0.36\n","| epoch   1 |  6000/13007 batches | loss  0.35\n","| epoch   1 |  6500/13007 batches | loss  0.34\n","| epoch   1 |  7000/13007 batches | loss  0.34\n","| epoch   1 |  7500/13007 batches | loss  0.34\n","| epoch   1 |  8000/13007 batches | loss  0.33\n","| epoch   1 |  8500/13007 batches | loss  0.32\n","| epoch   1 |  9000/13007 batches | loss  0.33\n","| epoch   1 |  9500/13007 batches | loss  0.32\n","| epoch   1 | 10000/13007 batches | loss  0.33\n","| epoch   1 | 10500/13007 batches | loss  0.31\n","| epoch   1 | 11000/13007 batches | loss  0.30\n","| epoch   1 | 11500/13007 batches | loss  0.30\n","| epoch   1 | 12000/13007 batches | loss  0.30\n","| epoch   1 | 12500/13007 batches | loss  0.30\n","| epoch   1 | 13000/13007 batches | loss  0.31\n","------------------------------------------------------------\n","| end of epoch   1 | accuracy on valid set:  0.90%\n","------------------------------------------------------------\n","| Model saved.\n","------------------------------------------------------------\n","| epoch   2 |   500/13007 batches | loss  0.29\n","| epoch   2 |  1000/13007 batches | loss  0.31\n","| epoch   2 |  1500/13007 batches | loss  0.30\n","| epoch   2 |  2000/13007 batches | loss  0.28\n","| epoch   2 |  2500/13007 batches | loss  0.29\n","| epoch   2 |  3000/13007 batches | loss  0.28\n","| epoch   2 |  3500/13007 batches | loss  0.29\n","| epoch   2 |  4000/13007 batches | loss  0.29\n","| epoch   2 |  4500/13007 batches | loss  0.29\n","| epoch   2 |  5000/13007 batches | loss  0.28\n","| epoch   2 |  5500/13007 batches | loss  0.29\n","| epoch   2 |  6000/13007 batches | loss  0.28\n","| epoch   2 |  6500/13007 batches | loss  0.27\n","| epoch   2 |  7000/13007 batches | loss  0.28\n","| epoch   2 |  7500/13007 batches | loss  0.27\n","| epoch   2 |  8000/13007 batches | loss  0.27\n","| epoch   2 |  8500/13007 batches | loss  0.28\n","| epoch   2 |  9000/13007 batches | loss  0.28\n","| epoch   2 |  9500/13007 batches | loss  0.27\n","| epoch   2 | 10000/13007 batches | loss  0.26\n","| epoch   2 | 10500/13007 batches | loss  0.27\n","| epoch   2 | 11000/13007 batches | loss  0.26\n","| epoch   2 | 11500/13007 batches | loss  0.28\n","| epoch   2 | 12000/13007 batches | loss  0.26\n","| epoch   2 | 12500/13007 batches | loss  0.27\n","| epoch   2 | 13000/13007 batches | loss  0.27\n","------------------------------------------------------------\n","| end of epoch   2 | accuracy on valid set:  0.91%\n","------------------------------------------------------------\n","| Model saved.\n","------------------------------------------------------------\n","| epoch   3 |   500/13007 batches | loss  0.26\n","| epoch   3 |  1000/13007 batches | loss  0.27\n","| epoch   3 |  1500/13007 batches | loss  0.26\n","| epoch   3 |  2000/13007 batches | loss  0.27\n","| epoch   3 |  2500/13007 batches | loss  0.26\n","| epoch   3 |  3000/13007 batches | loss  0.27\n","| epoch   3 |  3500/13007 batches | loss  0.26\n","| epoch   3 |  4000/13007 batches | loss  0.26\n","| epoch   3 |  4500/13007 batches | loss  0.26\n","| epoch   3 |  5000/13007 batches | loss  0.26\n","| epoch   3 |  5500/13007 batches | loss  0.26\n","| epoch   3 |  6000/13007 batches | loss  0.26\n","| epoch   3 |  6500/13007 batches | loss  0.26\n","| epoch   3 |  7000/13007 batches | loss  0.26\n","| epoch   3 |  7500/13007 batches | loss  0.27\n","| epoch   3 |  8000/13007 batches | loss  0.26\n","| epoch   3 |  8500/13007 batches | loss  0.26\n","| epoch   3 |  9000/13007 batches | loss  0.26\n","| epoch   3 |  9500/13007 batches | loss  0.26\n","| epoch   3 | 10000/13007 batches | loss  0.25\n","| epoch   3 | 10500/13007 batches | loss  0.24\n","| epoch   3 | 11000/13007 batches | loss  0.26\n","| epoch   3 | 11500/13007 batches | loss  0.25\n","| epoch   3 | 12000/13007 batches | loss  0.25\n","| epoch   3 | 12500/13007 batches | loss  0.25\n","| epoch   3 | 13000/13007 batches | loss  0.25\n","------------------------------------------------------------\n","| end of epoch   3 | accuracy on valid set:  0.91%\n","------------------------------------------------------------\n","| Model saved.\n","------------------------------------------------------------\n","| epoch   4 |   500/13007 batches | loss  0.25\n","| epoch   4 |  1000/13007 batches | loss  0.24\n","| epoch   4 |  1500/13007 batches | loss  0.25\n","| epoch   4 |  2000/13007 batches | loss  0.25\n","| epoch   4 |  2500/13007 batches | loss  0.25\n","| epoch   4 |  3000/13007 batches | loss  0.24\n","| epoch   4 |  3500/13007 batches | loss  0.25\n","| epoch   4 |  4000/13007 batches | loss  0.27\n","| epoch   4 |  4500/13007 batches | loss  0.25\n","| epoch   4 |  5000/13007 batches | loss  0.25\n","| epoch   4 |  5500/13007 batches | loss  0.25\n","| epoch   4 |  6000/13007 batches | loss  0.27\n","| epoch   4 |  6500/13007 batches | loss  0.23\n","| epoch   4 |  7000/13007 batches | loss  0.25\n","| epoch   4 |  7500/13007 batches | loss  0.25\n","| epoch   4 |  8000/13007 batches | loss  0.24\n","| epoch   4 |  8500/13007 batches | loss  0.25\n","| epoch   4 |  9000/13007 batches | loss  0.25\n","| epoch   4 |  9500/13007 batches | loss  0.25\n","| epoch   4 | 10000/13007 batches | loss  0.25\n","| epoch   4 | 10500/13007 batches | loss  0.24\n","| epoch   4 | 11000/13007 batches | loss  0.24\n","| epoch   4 | 11500/13007 batches | loss  0.24\n","| epoch   4 | 12000/13007 batches | loss  0.24\n","| epoch   4 | 12500/13007 batches | loss  0.24\n","| epoch   4 | 13000/13007 batches | loss  0.26\n","------------------------------------------------------------\n","| end of epoch   4 | accuracy on valid set:  0.92%\n","------------------------------------------------------------\n","| Model saved.\n","------------------------------------------------------------\n","| epoch   5 |   500/13007 batches | loss  0.23\n","| epoch   5 |  1000/13007 batches | loss  0.24\n","| epoch   5 |  1500/13007 batches | loss  0.24\n","| epoch   5 |  2000/13007 batches | loss  0.25\n","| epoch   5 |  2500/13007 batches | loss  0.25\n","| epoch   5 |  3000/13007 batches | loss  0.24\n","| epoch   5 |  3500/13007 batches | loss  0.24\n","| epoch   5 |  4000/13007 batches | loss  0.25\n","| epoch   5 |  4500/13007 batches | loss  0.23\n","| epoch   5 |  5000/13007 batches | loss  0.23\n","| epoch   5 |  5500/13007 batches | loss  0.23\n","| epoch   5 |  6000/13007 batches | loss  0.25\n","| epoch   5 |  6500/13007 batches | loss  0.22\n","| epoch   5 |  7000/13007 batches | loss  0.24\n","| epoch   5 |  7500/13007 batches | loss  0.25\n","| epoch   5 |  8000/13007 batches | loss  0.24\n","| epoch   5 |  8500/13007 batches | loss  0.24\n","| epoch   5 |  9000/13007 batches | loss  0.22\n","| epoch   5 |  9500/13007 batches | loss  0.23\n","| epoch   5 | 10000/13007 batches | loss  0.23\n","| epoch   5 | 10500/13007 batches | loss  0.24\n","| epoch   5 | 11000/13007 batches | loss  0.22\n","| epoch   5 | 11500/13007 batches | loss  0.24\n","| epoch   5 | 12000/13007 batches | loss  0.23\n","| epoch   5 | 12500/13007 batches | loss  0.25\n","| epoch   5 | 13000/13007 batches | loss  0.25\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U96f_Fhss5zq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"678a3618-3214-4e88-8e10-90b4e0f71082"},"source":["checkpoint = load()\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","current_epoch = checkpoint['epoch']\n","highest_accuracy = checkpoint['best_loss']\n","model.to(device)\n","\n","for epoch in range(1, EPOCHS + 1):\n","\n","    train()\n","    accuracy = evaluate(model, valid_loader)\n","    print_line()\n","    print('| end of epoch {:3d} | accuracy on valid set: {:5.2f}%'.format(\n","        epoch,\n","        accuracy)\n","    )\n","    print_line()\n","\n","    if accuracy > highest_accuracy:\n","        highest_accuracy = accuracy\n","        save(\n","             epoch, \n","             model.state_dict(), \n","             optimizer.state_dict(), \n","             highest_accuracy\n","        )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["| epoch   1 |   500/13007 batches | loss  0.24\n","| epoch   1 |  1000/13007 batches | loss  0.23\n","| epoch   1 |  1500/13007 batches | loss  0.25\n","| epoch   1 |  2000/13007 batches | loss  0.24\n","| epoch   1 |  2500/13007 batches | loss  0.26\n","| epoch   1 |  3000/13007 batches | loss  0.24\n","| epoch   1 |  3500/13007 batches | loss  0.24\n","| epoch   1 |  4000/13007 batches | loss  0.25\n","| epoch   1 |  4500/13007 batches | loss  0.24\n","| epoch   1 |  5000/13007 batches | loss  0.24\n","| epoch   1 |  5500/13007 batches | loss  0.24\n","| epoch   1 |  6000/13007 batches | loss  0.22\n","| epoch   1 |  6500/13007 batches | loss  0.24\n","| epoch   1 |  7000/13007 batches | loss  0.24\n","| epoch   1 |  7500/13007 batches | loss  0.23\n","| epoch   1 |  8000/13007 batches | loss  0.24\n","| epoch   1 |  8500/13007 batches | loss  0.25\n","| epoch   1 |  9000/13007 batches | loss  0.24\n","| epoch   1 |  9500/13007 batches | loss  0.25\n","| epoch   1 | 10000/13007 batches | loss  0.22\n","| epoch   1 | 10500/13007 batches | loss  0.24\n","| epoch   1 | 11000/13007 batches | loss  0.24\n","| epoch   1 | 11500/13007 batches | loss  0.23\n","| epoch   1 | 12000/13007 batches | loss  0.23\n","| epoch   1 | 12500/13007 batches | loss  0.22\n","| epoch   1 | 13000/13007 batches | loss  0.24\n","------------------------------------------------------------\n","| end of epoch   1 | accuracy on valid set:  0.92%\n","------------------------------------------------------------\n","| Model saved.\n","------------------------------------------------------------\n","| epoch   2 |   500/13007 batches | loss  0.23\n","| epoch   2 |  1000/13007 batches | loss  0.23\n","| epoch   2 |  1500/13007 batches | loss  0.24\n","| epoch   2 |  2000/13007 batches | loss  0.24\n","| epoch   2 |  2500/13007 batches | loss  0.23\n","| epoch   2 |  3000/13007 batches | loss  0.24\n","| epoch   2 |  3500/13007 batches | loss  0.24\n","| epoch   2 |  4000/13007 batches | loss  0.22\n","| epoch   2 |  4500/13007 batches | loss  0.24\n","| epoch   2 |  5000/13007 batches | loss  0.23\n","| epoch   2 |  5500/13007 batches | loss  0.23\n","| epoch   2 |  6000/13007 batches | loss  0.22\n","| epoch   2 |  6500/13007 batches | loss  0.23\n","| epoch   2 |  7000/13007 batches | loss  0.23\n","| epoch   2 |  7500/13007 batches | loss  0.22\n","| epoch   2 |  8000/13007 batches | loss  0.24\n","| epoch   2 |  8500/13007 batches | loss  0.23\n","| epoch   2 |  9000/13007 batches | loss  0.23\n","| epoch   2 |  9500/13007 batches | loss  0.24\n","| epoch   2 | 10000/13007 batches | loss  0.23\n","| epoch   2 | 10500/13007 batches | loss  0.23\n","| epoch   2 | 11000/13007 batches | loss  0.23\n","| epoch   2 | 11500/13007 batches | loss  0.22\n","| epoch   2 | 12000/13007 batches | loss  0.24\n","| epoch   2 | 12500/13007 batches | loss  0.22\n","| epoch   2 | 13000/13007 batches | loss  0.23\n","------------------------------------------------------------\n","| end of epoch   2 | accuracy on valid set:  0.92%\n","------------------------------------------------------------\n","| Model saved.\n","------------------------------------------------------------\n","| epoch   3 |   500/13007 batches | loss  0.23\n","| epoch   3 |  1000/13007 batches | loss  0.22\n","| epoch   3 |  1500/13007 batches | loss  0.23\n","| epoch   3 |  2000/13007 batches | loss  0.23\n","| epoch   3 |  2500/13007 batches | loss  0.22\n","| epoch   3 |  3000/13007 batches | loss  0.23\n","| epoch   3 |  3500/13007 batches | loss  0.24\n","| epoch   3 |  4000/13007 batches | loss  0.23\n","| epoch   3 |  4500/13007 batches | loss  0.22\n","| epoch   3 |  5000/13007 batches | loss  0.21\n","| epoch   3 |  5500/13007 batches | loss  0.23\n","| epoch   3 |  6000/13007 batches | loss  0.23\n","| epoch   3 |  6500/13007 batches | loss  0.23\n","| epoch   3 |  7000/13007 batches | loss  0.22\n","| epoch   3 |  7500/13007 batches | loss  0.22\n","| epoch   3 |  8000/13007 batches | loss  0.21\n","| epoch   3 |  8500/13007 batches | loss  0.21\n","| epoch   3 |  9000/13007 batches | loss  0.21\n","| epoch   3 |  9500/13007 batches | loss  0.22\n","| epoch   3 | 10000/13007 batches | loss  0.21\n","| epoch   3 | 10500/13007 batches | loss  0.22\n","| epoch   3 | 11000/13007 batches | loss  0.22\n","| epoch   3 | 11500/13007 batches | loss  0.22\n","| epoch   3 | 12000/13007 batches | loss  0.21\n","| epoch   3 | 12500/13007 batches | loss  0.23\n","| epoch   3 | 13000/13007 batches | loss  0.23\n","------------------------------------------------------------\n","| end of epoch   3 | accuracy on valid set:  0.93%\n","------------------------------------------------------------\n","| Model saved.\n","------------------------------------------------------------\n","| epoch   4 |   500/13007 batches | loss  0.21\n","| epoch   4 |  1000/13007 batches | loss  0.22\n","| epoch   4 |  1500/13007 batches | loss  0.22\n","| epoch   4 |  2000/13007 batches | loss  0.21\n","| epoch   4 |  2500/13007 batches | loss  0.22\n","| epoch   4 |  3000/13007 batches | loss  0.23\n","| epoch   4 |  3500/13007 batches | loss  0.20\n","| epoch   4 |  4000/13007 batches | loss  0.22\n","| epoch   4 |  4500/13007 batches | loss  0.22\n","| epoch   4 |  5000/13007 batches | loss  0.21\n","| epoch   4 |  5500/13007 batches | loss  0.20\n","| epoch   4 |  6000/13007 batches | loss  0.22\n","| epoch   4 |  6500/13007 batches | loss  0.21\n","| epoch   4 |  7000/13007 batches | loss  0.20\n","| epoch   4 |  7500/13007 batches | loss  0.20\n","| epoch   4 |  8000/13007 batches | loss  0.22\n","| epoch   4 |  8500/13007 batches | loss  0.21\n","| epoch   4 |  9000/13007 batches | loss  0.20\n","| epoch   4 |  9500/13007 batches | loss  0.20\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"spugmFLrLHRR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"a4c92c61-493e-44be-e33c-a82bb3059f54"},"source":["checkpoint = load()\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","current_epoch = checkpoint['epoch']\n","highest_accuracy = checkpoint['best_loss']\n","model.to(device)\n","\n","for epoch in range(1, EPOCHS + 1):\n","\n","    train()\n","    accuracy = evaluate(model, valid_loader)\n","    print_line()\n","    print('| end of epoch {:3d} | accuracy on valid set: {:5.2f}%'.format(\n","        epoch,\n","        accuracy)\n","    )\n","    print_line()\n","\n","    if accuracy > highest_accuracy:\n","        highest_accuracy = accuracy\n","        save(\n","             epoch, \n","             model.state_dict(), \n","             optimizer.state_dict(), \n","             highest_accuracy\n","        )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["| epoch   1 |   500/13007 batches | loss  0.21\n","| epoch   1 |  1000/13007 batches | loss  0.20\n","| epoch   1 |  1500/13007 batches | loss  0.20\n","| epoch   1 |  2000/13007 batches | loss  0.20\n","| epoch   1 |  2500/13007 batches | loss  0.21\n","| epoch   1 |  3000/13007 batches | loss  0.22\n","| epoch   1 |  3500/13007 batches | loss  0.21\n","| epoch   1 |  4000/13007 batches | loss  0.21\n","| epoch   1 |  4500/13007 batches | loss  0.21\n","| epoch   1 |  5000/13007 batches | loss  0.21\n","| epoch   1 |  5500/13007 batches | loss  0.20\n","| epoch   1 |  6000/13007 batches | loss  0.20\n","| epoch   1 |  6500/13007 batches | loss  0.21\n","| epoch   1 |  7000/13007 batches | loss  0.20\n","| epoch   1 |  7500/13007 batches | loss  0.21\n","| epoch   1 |  8000/13007 batches | loss  0.21\n","| epoch   1 |  8500/13007 batches | loss  0.21\n","| epoch   1 |  9000/13007 batches | loss  0.21\n","| epoch   1 |  9500/13007 batches | loss  0.21\n","| epoch   1 | 10000/13007 batches | loss  0.21\n","| epoch   1 | 10500/13007 batches | loss  0.21\n","| epoch   1 | 11000/13007 batches | loss  0.19\n","| epoch   1 | 11500/13007 batches | loss  0.21\n","| epoch   1 | 12000/13007 batches | loss  0.21\n","| epoch   1 | 12500/13007 batches | loss  0.20\n","| epoch   1 | 13000/13007 batches | loss  0.20\n","------------------------------------------------------------\n","| end of epoch   1 | accuracy on valid set:  0.93%\n","------------------------------------------------------------\n","| Model saved.\n","------------------------------------------------------------\n","| epoch   2 |   500/13007 batches | loss  0.21\n","| epoch   2 |  1000/13007 batches | loss  0.21\n","| epoch   2 |  1500/13007 batches | loss  0.21\n","| epoch   2 |  2000/13007 batches | loss  0.20\n","| epoch   2 |  2500/13007 batches | loss  0.20\n","| epoch   2 |  3000/13007 batches | loss  0.21\n","| epoch   2 |  3500/13007 batches | loss  0.20\n","| epoch   2 |  4000/13007 batches | loss  0.21\n","| epoch   2 |  4500/13007 batches | loss  0.21\n","| epoch   2 |  5000/13007 batches | loss  0.21\n","| epoch   2 |  5500/13007 batches | loss  0.20\n","| epoch   2 |  6000/13007 batches | loss  0.20\n","| epoch   2 |  6500/13007 batches | loss  0.18\n","| epoch   2 |  7000/13007 batches | loss  0.20\n","| epoch   2 |  7500/13007 batches | loss  0.22\n","| epoch   2 |  8000/13007 batches | loss  0.21\n","| epoch   2 |  8500/13007 batches | loss  0.22\n","| epoch   2 |  9000/13007 batches | loss  0.20\n","| epoch   2 |  9500/13007 batches | loss  0.20\n","| epoch   2 | 10000/13007 batches | loss  0.19\n","| epoch   2 | 10500/13007 batches | loss  0.21\n","| epoch   2 | 11000/13007 batches | loss  0.20\n","| epoch   2 | 11500/13007 batches | loss  0.20\n","| epoch   2 | 12000/13007 batches | loss  0.21\n","| epoch   2 | 12500/13007 batches | loss  0.21\n","| epoch   2 | 13000/13007 batches | loss  0.19\n","------------------------------------------------------------\n","| end of epoch   2 | accuracy on valid set:  0.94%\n","------------------------------------------------------------\n","| Model saved.\n","------------------------------------------------------------\n","| epoch   3 |   500/13007 batches | loss  0.19\n","| epoch   3 |  1000/13007 batches | loss  0.21\n","| epoch   3 |  1500/13007 batches | loss  0.21\n","| epoch   3 |  2000/13007 batches | loss  0.19\n","| epoch   3 |  2500/13007 batches | loss  0.19\n","| epoch   3 |  3000/13007 batches | loss  0.20\n","| epoch   3 |  3500/13007 batches | loss  0.20\n","| epoch   3 |  4000/13007 batches | loss  0.19\n","| epoch   3 |  4500/13007 batches | loss  0.20\n","| epoch   3 |  5000/13007 batches | loss  0.20\n","| epoch   3 |  5500/13007 batches | loss  0.20\n","| epoch   3 |  6000/13007 batches | loss  0.20\n","| epoch   3 |  6500/13007 batches | loss  0.21\n","| epoch   3 |  7000/13007 batches | loss  0.20\n","| epoch   3 |  7500/13007 batches | loss  0.19\n","| epoch   3 |  8000/13007 batches | loss  0.19\n","| epoch   3 |  8500/13007 batches | loss  0.21\n","| epoch   3 |  9000/13007 batches | loss  0.20\n","| epoch   3 |  9500/13007 batches | loss  0.18\n","| epoch   3 | 10000/13007 batches | loss  0.20\n","| epoch   3 | 10500/13007 batches | loss  0.20\n","| epoch   3 | 11000/13007 batches | loss  0.19\n","| epoch   3 | 11500/13007 batches | loss  0.19\n","| epoch   3 | 12000/13007 batches | loss  0.21\n","| epoch   3 | 12500/13007 batches | loss  0.20\n","| epoch   3 | 13000/13007 batches | loss  0.19\n","------------------------------------------------------------\n","| end of epoch   3 | accuracy on valid set:  0.94%\n","------------------------------------------------------------\n","| Model saved.\n","------------------------------------------------------------\n","| epoch   4 |   500/13007 batches | loss  0.20\n","| epoch   4 |  1000/13007 batches | loss  0.20\n","| epoch   4 |  1500/13007 batches | loss  0.19\n","| epoch   4 |  2000/13007 batches | loss  0.19\n","| epoch   4 |  2500/13007 batches | loss  0.19\n","| epoch   4 |  3000/13007 batches | loss  0.19\n","| epoch   4 |  3500/13007 batches | loss  0.19\n","| epoch   4 |  4000/13007 batches | loss  0.18\n","| epoch   4 |  4500/13007 batches | loss  0.20\n","| epoch   4 |  5000/13007 batches | loss  0.19\n","| epoch   4 |  5500/13007 batches | loss  0.20\n","| epoch   4 |  6000/13007 batches | loss  0.19\n","| epoch   4 |  6500/13007 batches | loss  0.20\n","| epoch   4 |  7000/13007 batches | loss  0.19\n","| epoch   4 |  7500/13007 batches | loss  0.21\n","| epoch   4 |  8000/13007 batches | loss  0.21\n","| epoch   4 |  8500/13007 batches | loss  0.20\n","| epoch   4 |  9000/13007 batches | loss  0.20\n","| epoch   4 |  9500/13007 batches | loss  0.20\n","| epoch   4 | 10000/13007 batches | loss  0.18\n","| epoch   4 | 10500/13007 batches | loss  0.20\n","| epoch   4 | 11000/13007 batches | loss  0.18\n","| epoch   4 | 11500/13007 batches | loss  0.20\n","| epoch   4 | 12000/13007 batches | loss  0.19\n","| epoch   4 | 12500/13007 batches | loss  0.20\n","| epoch   4 | 13000/13007 batches | loss  0.20\n","------------------------------------------------------------\n","| end of epoch   4 | accuracy on valid set:  0.94%\n","------------------------------------------------------------\n","| Model saved.\n","------------------------------------------------------------\n","| epoch   5 |   500/13007 batches | loss  0.21\n","| epoch   5 |  1000/13007 batches | loss  0.19\n","| epoch   5 |  1500/13007 batches | loss  0.19\n","| epoch   5 |  2000/13007 batches | loss  0.18\n","| epoch   5 |  2500/13007 batches | loss  0.18\n","| epoch   5 |  3000/13007 batches | loss  0.20\n","| epoch   5 |  3500/13007 batches | loss  0.20\n","| epoch   5 |  4000/13007 batches | loss  0.18\n","| epoch   5 |  4500/13007 batches | loss  0.18\n","| epoch   5 |  5000/13007 batches | loss  0.21\n","| epoch   5 |  5500/13007 batches | loss  0.19\n","| epoch   5 |  6000/13007 batches | loss  0.19\n","| epoch   5 |  6500/13007 batches | loss  0.19\n","| epoch   5 |  7000/13007 batches | loss  0.18\n","| epoch   5 |  7500/13007 batches | loss  0.20\n","| epoch   5 |  8000/13007 batches | loss  0.19\n","| epoch   5 |  8500/13007 batches | loss  0.19\n","| epoch   5 |  9000/13007 batches | loss  0.19\n","| epoch   5 |  9500/13007 batches | loss  0.20\n","| epoch   5 | 10000/13007 batches | loss  0.20\n","| epoch   5 | 10500/13007 batches | loss  0.18\n","| epoch   5 | 11000/13007 batches | loss  0.19\n","| epoch   5 | 11500/13007 batches | loss  0.20\n","| epoch   5 | 12000/13007 batches | loss  0.19\n","| epoch   5 | 12500/13007 batches | loss  0.19\n","| epoch   5 | 13000/13007 batches | loss  0.18\n","------------------------------------------------------------\n","| end of epoch   5 | accuracy on valid set:  0.94%\n","------------------------------------------------------------\n","| Model saved.\n","------------------------------------------------------------\n","| epoch   6 |   500/13007 batches | loss  0.19\n","| epoch   6 |  1000/13007 batches | loss  0.19\n","| epoch   6 |  1500/13007 batches | loss  0.20\n","| epoch   6 |  2000/13007 batches | loss  0.20\n","| epoch   6 |  2500/13007 batches | loss  0.20\n","| epoch   6 |  3000/13007 batches | loss  0.20\n","| epoch   6 |  3500/13007 batches | loss  0.19\n","| epoch   6 |  4000/13007 batches | loss  0.18\n","| epoch   6 |  4500/13007 batches | loss  0.19\n"],"name":"stdout"}]}]}